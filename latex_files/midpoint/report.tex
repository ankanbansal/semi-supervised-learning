% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
% Based on CVPR 07 and LNCS, with modifications by DAF, AZ and elle, 2008 and AA, 2010, and CC, 2011; TT, 2014; AAS, 2016; AAS 2018

\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{color}
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Semi-Supervised Learning....} 
% Replace with your title

\titlerunning{Bansal, Zepeda}
% Replace with a meaningful short version of your title
%
\author{Ankan Bansal~~~~~Joaquin Zepeda}

%\institute{UMD \and Amazon}
%
\maketitle              % typeset the header of the contribution

\section{Problem}
Getting annotations is difficult. We want to leverage the large amount of unlabeled data in addition
to the existing labeled data for training training deep networks. 

Closest work?


\section{Idea}

$\mathbf{q_t}$ is the output probability distribution over the classes. $T$ is the batch size.
$\mathcal{L}_{cls}$ is the cross-entropy loss. 

\subsection{Mean Entropy Penalty (MEL)}
\begin{equation}
	\mathcal{L}_{MEL} = \frac{1}{T}\sum_{t=1}^{T}H(\mathbf{q_t})
\end{equation}
where $H(.)$ represents the entropy. 

\subsection{Negative Batch Entropy Penalty (NBEL)}
\begin{equation}
	\mathcal{L}_{NBEL} = -H(\frac{1}{T}\sum_{t=1}^{T}\mathbf{q_t})
\end{equation}


\subsection{Locality Penalty}
Class Activation Maps, $C$. Feature maps, $F$. Weights of the final layer, $W$. $C = A \times W$.
The class with maximum probability is $i$. Calculate locality loss on $C_i$. 
\begin{equation}
	\mathcal{L}_{Loc} = \sum_{j=1}^{4}l_j(C_i)
\end{equation}
where the sum is over the four groups (left-right, right-left, top-bottom, and bottom-top).

Consider one of these cases. Let, $C_i \in \mathbb{R}^{N \times N}$. We define the left-right group
norms as $G_{lr} = [\left\lVert C^{k}_i \right\rVert_2~|~C^{k}_i = C_i(1:k,1:N), k = 1 \dots N]$. Then,
$l_1(C_i) = \left\lVert G_{lr} \right\rVert_1$. Other group losses can be defined similarly.\\

Then the total loss is given as:
\begin{equation}
	\mathcal{L} = \mathcal{L}_{cls} + \alpha \mathcal{L}_{MEL} + \beta \mathcal{L}_{NBEL} + \gamma
	\mathcal{L}_{Loc}
\end{equation}


\section{Evaluation}
We use ImageNet as the training and testing dataset. We fix a set of about 64,000 images for which
we have labels available (supervised set). We use an additional 200,000 images as unsupervised
images, i.e. images for which labels are not available. We use the ImageNet validation set as the
test set. We use the state-of-the-art DenseNet model.

\subsection{Baseline}
We train a network with only the supervised images and only supervised images.

\subsection{Cross-entropy + MEL}

We noticed that the performance of the model was close to the baseline but not quite there. When we
had tried this case (cross-entropy + MEL) with only supervised data, we had achieved small
improvements over baseline. This led us to give a higher weight to supervised data while sampling a
batch. 

We varied the ratio of supervised images in  a batch from the default (0.25) to 1.0. The performance
keeps improving till 0.8. But from 0.9, over-fitting becomes a problem. A concern could be that the
gain in performance might be because of passing the supervised data so many times. However, we
eliminated this.

\subsection{Cross-entropy + MEL + NBEL}
Now we add the negative batch-entropy loss to cross-entropy loss and MEL. This loss is based on the
idea that uniform sampling from the dataset should lead to uniform sampling over the output classes.
We varied $\beta$ while keeping $\alpha$ and $r$ fixed from the previous case. 


\subsection{Cross-entropy + Loc}
We vary $\gamma$.

We are also currently trying to weigh the class activation maps by the corresponding probabilities
instead of using only the CAM with the maximum probability. 


\subsection{Cross-entropy + MEL + BEL + Loc}
TODO





% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%
\begin{thebibliography}{8}
\bibitem{ref_article1}
Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)

\end{thebibliography}
\end{document}
