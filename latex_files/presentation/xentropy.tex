\subsection{Cross Entropy and Clustering}
\begin{frame}
	\frametitle{Adding the Supervised Loss}
	\begin{itemize}
		\item Consider a mini-batch, 
			\begin{block}{Mini-batch}
				\begin{equation*}
					\mathcal{B} = \{(\mathbf{X}_k, y_k)\}_{k=1}^R \cup \{\mathbf{X}_k\}_{k=R+1}^T
				\end{equation*}
			\end{block}
		\item The sum of supervised and clustering losses is:
			\begin{block}{Cross Entropy + Clustering}
				\begin{equation*}
					\mathcal{L} = \underbrace{\frac{1}{R} \sum_{t=1}^{R} E_t}_{J_C} ~+~ \alpha
					\underbrace{\frac{1}{T}\sum_{t=1}^{T}H(\mathbf{q}_t)}_{J_M} ~+~
					\beta \underbrace{D_{KL}(\frac{1}{T}\sum_{t=1}^{T}\mathbf{q}_t \lVert
					\mathbf{d})}_{J_B}
				\end{equation*}
				where $E_t$ is the cross-entropy loss for image $\mathbf{X}_t$
			\end{block}
	\end{itemize}
\end{frame}

