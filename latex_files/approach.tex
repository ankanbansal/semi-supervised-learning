\section{Proposed Approach}

% The following has been copied from the midpoint report
We have two sets of images, one with labels, $\mathcal{S} = \{(\mathbf{X}_i, y_i)\}_{i=1}^S$, and 
the other without labels, $\mathcal{U} = \{\mathbf{X}_j\}_{j=S+1}^{S+U}$. Here, $\mathbf{X}_i$ is the
$i^{th}$ image and $y_i \in \mathcal{C}$ is the corresponding label and $\mathcal{C}$ is the set of
classes.  Using this data, we want to train an image classifier, $f(\mathbf{X}; \Theta)$,
which takes an image as input and outputs the probability distribution, $\mathbf{q}$, over classes
$\mathcal{C}$.

\subsection{Clustering Losses}

The first idea is based on unsupervised clustering of images. We can cluster the images into
$|\mathcal{C}|$ clusters. However, without any labels, the clusters do not necessarily
correspond to semantic classes. We can use the labeled samples $\mathcal{S}$ to seed the clusters.
This ensures that the clusters formed have a semantic meaning.

Suppose that the probability distribution of labels in the dataset is $\mathbf{d}$. This can be
easily obtained from the labeled set, $\mathcal{S}$.  Let $H(.)$ represent the entropy of a
probability distribution, defined as $H(\mathbf{p}) = -\sum_{i=1}^{C}p_i \log(p_i)$.  Say that, for
an image $\mathbf{X}_t$, the output of the classifier is a probability distribution, $\mathbf{q}_t$,
over the classes $\mathcal{C}$. We draw a batch of size $T$ and compute the following two losses
over the batch \cite{subic}.\\
\newline
\textbf{Mean Entropy Loss (MEL)}
\begin{equation}
	J_M = \frac{1}{T}\sum_{t=1}^{T}H(\mathbf{q}_t)
\end{equation}
\textbf{Negative Batch Entropy Loss (NBEL) (Need to come up with a new name now)}
\begin{equation}
	\label{eq:nbel}
	J_B = D_{KL} (\frac{1}{T}\sum_{t=1}^{T}\mathbf{q}_t \lVert \mathbf{d})
\end{equation}
where $D_{KL}(\mathbf{a} \lVert	\mathbf{b})$ is the KL-divergence between the probability
distributions $\mathbf{a}$ and $\mathbf{b}$, given as $D_{KL}(\mathbf{a} \lVert \mathbf{b}) =
\sum_{i=1}^{C}a_i \log(a_i / b_i)$. Note that for datasets which have a uniform distribution of
classes, like ImageNet and CIFAR-10, (\ref{eq:nbel}) simply reduces to the negative batch
entropy loss (NBEL):
\begin{equation}
	J_B = -H(\frac{1}{T}\sum_{t=1}^{T}\mathbf{q}_t)
\end{equation}

The first (MEL) tries to increase the confidence of the predicted class and the second (NBEL)
ensures that the predictions do not collapse to a single label and are spread out similar to
the actual label distribution. NBEL is based on the assumption that, when classes are distributed
according to $\mathbf{d}$, uniform sampling from the dataset should lead to the same distribution
over the output classes.

For a supervised image $\mathbf X$, we are given a ground-truth probability distribution, $\mathbf{p}$ (This is
just a one-hot probability vector in our case). The objective of a classifier, $f$, is to minimize the
cross-entropy $E = -\sum_{i=1}^{C}p_i \log(q_i)$. Here, $p_i$ and $q_i$
are the elements of $\mathbf{p}$ and $\mathbf{q} = f(\mathbf{X}; \Theta)$ respectively. For a
mini-batch of size $T$, the cross-entropy loss can be defined as $J_C = \frac{1}{T} \sum_{t=1}^{T}
E_t$, where $E_t$ is the cross-entropy for image $\mathbf{X}_t$.

Consider a mini-batch, $\mathcal{B} =
\{(\mathbf{X}_k, y_k)\}_{k=1}^R \cup \{\mathbf{X}_k\}_{k=R+1}^T$, where $\{(\mathbf{X}_k,
k_i)\}_{k=1}^{R} \subseteq \mathcal{S}$, and $\{\mathbf{X}_k\}_{k=R+1}^{T} \subseteq \mathcal{U}$. Our total loss
is given as:
\begin{equation}
	\mathcal{L} = J_C + \alpha J_M + \beta J_B
\end{equation}
\begin{equation}
	\mathcal{L} = \frac{1}{R} \sum_{t=1}^{R} E_t + \alpha \frac{1}{T}\sum_{t=1}^{T}H(\mathbf{q}_t) +
	\beta D_{KL}(\frac{1}{T}\sum_{t=1}^{T}\mathbf{q}_t \lVert \mathbf{d})
\end{equation}

%\begin{equation}
%	\mathcal{L} = \frac{1}{R} \sum_{t=1}^{R} E_t + \alpha \frac{1}{T}\sum_{t=1}^{T}H(\mathbf{q}_t) -
%	\beta H(\frac{1}{T}\sum_{t=1}^{T}\mathbf{q}_t)
%\end{equation}

We find $\alpha$, $\beta$, and $R$ by cross-validation. The ratio $\frac{R}{T}$ can be thought of as
corresponding to the relative importance of supervised and unsupervised losses and data. A high
$\frac{R}{T}$ indicates that the supervised data and losses are considered more important than their
unsupervised counterparts. 


\subsection{Locality Loss}
The second idea is using a structured sparsity loss. This is a way of incorporating the prior knowledge
that an object occupies a small region in the image. We use the sparsity inducing norms introduced in
\cite{groupsparsity,sparsepca}. Consider a matrix, $C \in \mathbb{R}^{N \times
N}$. Let us define a subset of the power-set of the support of $C$ as
\begin{equation}
	\mathcal{G} = \{g_i \subseteq \textrm{support}(C)\}_i
\end{equation}
where $\textrm{support}(C) = [1 \dots N] \times [1 \dots N]$ and $\cup_{g \in \mathcal{G}} = \textrm{support}(C)$. The following norm, when used as a
loss, behaves like an $L$-$1$ norm on the group level and therefore, induces group sparsity:
\begin{equation}
	\Omega (C, \mathcal{G}) = \sum_{g \in \mathcal{G}} \left\lVert C_g \right\rVert _2,
\end{equation}
where $C_g$ is the vector formed by the values in $C$ indexed by $g$ and $\left\lVert . \right\rVert_2$ is
the $L$-$2$ norm. This loss encourages each $C_g$ to go to zero. However, within $C_g,~g
\in \mathcal{G}$, the $L$-$2$ norm does not promote sparsity.

We use four sets $\mathcal G_i = \{g_{ik}\}_{k=1}^N$ corresponding, respectively, to nested
left-to-right and right-to-left colums, and nested top-to-bottom and bottom-to-top columns,
\textit{i.e.,}
\begin{align}
  g_{1k} & = [1 \ldots N] \times [1 \ldots k] \\
  g_{2k} & = [1 \ldots N] \times [N-k+1 \ldots N] \\
  g_{3k} & = [1 \ldots k] \times [1 \ldots N] \\
  g_{4k} & = [N-k+1 \ldots N] \times [1 \ldots N]
\end{align}
Accordingly, we define the locality-inducing penalty for class $i$ of an image $\mathbf{X}_t$ as follows:
\begin{equation} \label{eq:JLt}
	J_{L,t}^{i} = \sum_{j=1}^{4}\Omega(C^i_t, \mathcal G_j),
\end{equation}
where $C^i_{t}$ is the class activation map \cite{CAM} derived from image $\mathbf X_t$ for class
$i$. Given some estimate $\mathbf q_{t}$ of the likelihood of a given class $i$ in image $\mathbf
X_t$, we define our locality-inducing penalty to be
\begin{equation}
  J_{L,t} = \sum_{i} q_{ti} J_{L,t}^i
\end{equation}
and total locality penalty for a batch of size $T$ as $J_L =  \frac{1}{T} \sum_{t=1}^T J_L^t$. 

Adding \eqref{eq:JLt} to the earlier loss formulation, the total loss is given as:
\begin{equation}
	\label{eq:total}
	\mathcal{L} = J_C + \alpha J_M + \beta J_B + \gamma J_L
\end{equation}
Again, for a batch, $J_C$ is calculated only for the supervised images, and $J_M, J_B, $ and $J_L$
are calculated for both supervised and unsupervised images.
