@article{Chrabaszcz2017,
	abstract = {The original ImageNet dataset is a popular large-scale benchmark for training Deep Neural Networks. Since the cost of performing experiments (e.g, algorithm design, architecture search, and hyperparameter tuning) on the original dataset might be prohibitive, we propose to consider a downsampled version of ImageNet. In contrast to the CIFAR datasets and earlier downsampled versions of ImageNet, our proposed ImageNet32{\$}\backslashtimes{\$}32 (and its variants ImageNet64{\$}\backslashtimes{\$}64 and ImageNet16{\$}\backslashtimes{\$}16) contains exactly the same number of classes and images as ImageNet, with the only difference that the images are downsampled to 32{\$}\backslashtimes{\$}32 pixels per image (64{\$}\backslashtimes{\$}64 and 16{\$}\backslashtimes{\$}16 pixels for the variants, respectively). Experiments on these downsampled variants are dramatically faster than on the original ImageNet and the characteristics of the downsampled datasets with respect to optimal hyperparameters appear to remain similar. The proposed datasets and scripts to reproduce our results are available at http://image-net.org/download-images and https://github.com/PatrykChrabaszcz/Imagenet32{\_}Scripts},
	archivePrefix = {arXiv},
	arxivId = {1707.08819},
	author = {Chrabaszcz, Patryk and Loshchilov, Ilya and Hutter, Frank},
	eprint = {1707.08819},
	title = {{A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets}},
	url = {http://image-net.org/download-images https://github.com/PatrykChrabaszcz/Imagenet32{\_}Scripts http://arxiv.org/abs/1707.08819},
	year = {2017}
}
@article{Su2016,
	abstract = {Model compression and knowledge distillation have been successfully applied for cross-architecture and cross-domain transfer learning. However, a key requirement is that training examples are in correspondence across the domains. We show that in many scenarios of practical importance such aligned data can be synthetically generated using computer graphics pipelines allowing domain adaptation through distillation. We apply this technique to learn models for recognizing low-resolution images using labeled high-resolution images, non-localized objects using labeled localized objects, line-drawings using labeled color images, etc. Experiments on various fine-grained recognition datasets demonstrate that the technique improves recognition performance on the low-quality data and beats strong baselines for domain adaptation. Finally, we present insights into workings of the technique through visualizations and relating it to existing literature.},
	archivePrefix = {arXiv},
	arxivId = {1604.00433},
	author = {Su, Jong-Chyi and Maji, Subhransu},
	eprint = {1604.00433},
	month = {apr},
	title = {{Adapting Models to Signal Degradation using Distillation}},
	url = {http://arxiv.org/abs/1604.00433},
	year = {2016}
}
@article{Zhang2018,
	abstract = {In this work, we propose Adversarial Complementary Learning (ACoL) to automatically localize integral objects of semantic interest with weak supervision. We first mathematically prove that class localization maps can be obtained by directly selecting the class-specific feature maps of the last convolutional layer, which paves a simple way to identify object regions. We then present a simple network architecture including two parallel-classifiers for object localization. Specifically, we leverage one classification branch to dynamically localize some discriminative object regions during the forward pass. Although it is usually responsive to sparse parts of the target objects, this classifier can drive the counterpart classifier to discover new and complementary object regions by erasing its discovered regions from the feature maps. With such an adversarial learning, the two parallel-classifiers are forced to leverage complementary object regions for classification and can finally generate integral object localization together. The merits of ACoL are mainly two-fold: 1) it can be trained in an end-to-end manner; 2) dynamically erasing enables the counterpart classifier to discover complementary object regions more effectively. We demonstrate the superiority of our ACoL approach in a variety of experiments. In particular, the Top-1 localization error rate on the ILSVRC dataset is 45.14{\%}, which is the new state-of-the-art.},
	archivePrefix = {arXiv},
	arxivId = {1804.06962},
	author = {Zhang, Xiaolin and Wei, Yunchao and Feng, Jiashi and Yang, Yi and Huang, Thomas},
	eprint = {1804.06962},
	file = {:Users/ankan/Desktop/Papers/Adversarial Complementary Learning for Weakly Supervised Object Localization.pdf:pdf},
	title = {{Adversarial Complementary Learning for Weakly Supervised Object Localization}},
	url = {http://arxiv.org/abs/1804.06962},
	year = {2018}
}
@article{Maal√∏e2016,
	abstract = {Deep generative models parameterized by neural networks have recently achieved state-of-the-art performance in unsupervised and semi-supervised learning. We extend deep generative models with auxiliary variables which improves the variational approximation. The auxiliary variables leave the generative model unchanged but make the variational distribution more expressive. Inspired by the structure of the auxiliary variable we also propose a model with two stochastic layers and skip connections. Our findings suggest that more expressive and properly specified deep generative models converge faster with better results. We show state-of-the-art performance within semi-supervised learning on MNIST, SVHN and NORB datasets.},
	archivePrefix = {arXiv},
	arxivId = {1602.05473},
	author = {Maal{\o}e, Lars and S{\o}nderby, Casper Kaae and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
	doi = {10.1109/ICCV.2009.5459469},
	eprint = {1602.05473},
	file = {:Users/ankan/Desktop/Papers/Semi-Supervised/Auxiliary Deep Generative Models.pdf:pdf},
	isbn = {1595937935},
	issn = {10495258},
	pmid = {25719670},
	title = {{Auxiliary Deep Generative Models}},
	url = {http://arxiv.org/abs/1602.05473},
	volume = {48},
	year = {2016}
}
@article{Noroozi2018,
	abstract = {In self-supervised learning, one trains a model to solve a so-called pretext task on a dataset without the need for human annotation. The main objective, however, is to transfer this model to a target domain and task. Currently, the most effective transfer strategy is fine-tuning, which restricts one to use the same model or parts thereof for both pretext and target tasks. In this paper, we present a novel framework for self-supervised learning that overcomes limitations in designing and comparing different tasks, models, and data domains. In particular, our framework decouples the structure of the self-supervised model from the final task-specific fine-tuned model. This allows us to: 1) quantitatively assess previously incompatible models including handcrafted features; 2) show that deeper neural network models can learn better representations from the same pretext task; 3) transfer knowledge learned with a deep model to a shallower one and thus boost its learning. We use this framework to design a novel self-supervised task, which achieves state-of-the-art performance on the common benchmarks in PASCAL VOC 2007, ILSVRC12 and Places by a significant margin. Our learned features shrink the mAP gap between models trained via self-supervised learning and supervised learning from 5.9{\%} to 2.6{\%} in object detection on PASCAL VOC 2007.},
	archivePrefix = {arXiv},
	arxivId = {1805.00385},
	author = {Noroozi, Mehdi and Vinjimoor, Ananth and Favaro, Paolo and Pirsiavash, Hamed},
	eprint = {1805.00385},
	file = {:Users/ankan/Desktop/Papers/Unsupervised/Boosting Self-Supervised Learning via Knowledge Transfer.pdf:pdf},
	title = {{Boosting Self-Supervised Learning via Knowledge Transfer}},
	url = {http://arxiv.org/abs/1805.00385},
	year = {2018}
}
@article{Pathak2016,
	abstract = {We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.},
	archivePrefix = {arXiv},
	arxivId = {1604.07379},
	author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A.},
	doi = {10.1109/CVPR.2016.278},
	eprint = {1604.07379},
	file = {:Users/ankan/Desktop/Papers/Unsupervised/Context Encoders- Feature Learning by Inpainting.pdf:pdf},
	isbn = {978-1-4673-8851-1},
	issn = {10636919},
	pmid = {4520227},
	title = {{Context Encoders: Feature Learning by Inpainting}},
	url = {http://arxiv.org/abs/1604.07379},
	year = {2016}
}
@article{Gupta2015,
	abstract = {In this work we propose a technique that transfers supervision between images from different modalities. We use learned representations from a large labeled modality as a supervisory signal for training representations for a new unlabeled paired modality. Our method enables learning of rich representations for unlabeled modalities and can be used as a pre-training procedure for new modalities with limited labeled data. We show experimental results where we transfer supervision from labeled RGB images to unlabeled depth and optical flow images and demonstrate large improvements for both these cross modal supervision transfers. Code, data and pre-trained models are available at https://github.com/s-gupta/fast-rcnn/tree/distillation},
	archivePrefix = {arXiv},
	arxivId = {1507.00448},
	author = {Gupta, Saurabh and Hoffman, Judy and Malik, Jitendra},
	doi = {10.1109/CVPR.2016.309},
	eprint = {1507.00448},
	isbn = {978-1-4673-8851-1},
	issn = {10636919},
	title = {{Cross Modal Distillation for Supervision Transfer}},
	url = {http://arxiv.org/abs/1507.00448},
	year = {2015}
}
@article{Gupta2015a,
	abstract = {In this work we propose a technique that transfers supervision between images from different modalities. We use learned representations from a large labeled modality as a supervisory signal for training representations for a new unlabeled paired modality. Our method enables learning of rich representations for unlabeled modalities and can be used as a pre-training procedure for new modalities with limited labeled data. We show experimental results where we transfer supervision from labeled RGB images to unlabeled depth and optical flow images and demonstrate large improvements for both these cross modal supervision transfers. Code, data and pre-trained models are available at https://github.com/s-gupta/fast-rcnn/tree/distillation},
	archivePrefix = {arXiv},
	arxivId = {1507.00448},
	author = {Gupta, Saurabh and Hoffman, Judy and Malik, Jitendra},
	doi = {10.1109/CVPR.2016.309},
	eprint = {1507.00448},
	isbn = {978-1-4673-8851-1},
	issn = {10636919},
	title = {{Cross Modal Distillation for Supervision Transfer}},
	url = {http://arxiv.org/abs/1507.00448},
	year = {2015}
}
@article{Inoue2018,
	abstract = {Can we detect common objects in a variety of image domains without instance-level annotations? In this paper, we present a framework for a novel task, cross-domain weakly supervised object detection, which addresses this question. For this paper, we have access to images with instance-level annotations in a source domain (e.g., natural image) and images with image-level annotations in a target domain (e.g., watercolor). In addition, the classes to be detected in the target domain are all or a subset of those in the source domain. Starting from a fully supervised object detector, which is pre-trained on the source domain, we propose a two-step progressive domain adaptation technique by fine-tuning the detector on two types of artificially and automatically generated samples. We test our methods on our newly collected datasets containing three image domains, and achieve an improvement of approximately 5 to 20 percentage points in terms of mean average precision (mAP) compared to the best-performing baselines.},
	archivePrefix = {arXiv},
	arxivId = {1803.11365},
	author = {Inoue, Naoto and Furuta, Ryosuke and Yamasaki, Toshihiko and Aizawa, Kiyoharu},
	eprint = {1803.11365},
	file = {:Users/ankan/Desktop/Papers/Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation.pdf:pdf},
	month = {mar},
	title = {{Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation}},
	url = {http://arxiv.org/abs/1803.11365},
	year = {2018}
}
@article{Radosavovic2017,
	abstract = {We investigate omni-supervised learning, a special regime of semi-supervised learning in which the learner exploits all available labeled data plus internet-scale sources of unlabeled data. Omni-supervised learning is lower-bounded by performance on existing labeled datasets, offering the potential to surpass state-of-the-art fully supervised methods. To exploit the omni-supervised setting, we propose data distillation, a method that ensembles predictions from multiple transformations of unlabeled data, using a single model, to automatically generate new training annotations. We argue that visual recognition models have recently become accurate enough that it is now possible to apply classic ideas about self-training to challenging real-world data. Our experimental results show that in the cases of human keypoint detection and general object detection, state-of-the-art models trained with data distillation surpass the performance of using labeled data from the COCO dataset alone.},
	archivePrefix = {arXiv},
	arxivId = {1712.04440},
	author = {Radosavovic, Ilija and Doll{\'{a}}r, Piotr and Girshick, Ross and Gkioxari, Georgia and He, Kaiming},
	eprint = {1712.04440},
	file = {:Users/ankan/Desktop/Papers/Semi-Supervised/Data Distillation- Towards Omni-Supervised Learning.pdf:pdf},
	title = {{Data Distillation: Towards Omni-Supervised Learning}},
	url = {http://arxiv.org/abs/1712.04440},
	year = {2017}
}
@article{Krahenbuhl2015,
	abstract = {Convolutional Neural Networks spread through computer vision like a wildfire, impacting almost all visual tasks imaginable. Despite this, few researchers dare to train their models from scratch. Most work builds on one of a handful of ImageNet pre-trained models, and fine-tunes or adapts these for specific tasks. This is in large part due to the difficulty of properly initializing these networks from scratch. A small miscalibration of the initial weights leads to vanishing or exploding gradients, as well as poor convergence properties. In this work we present a fast and simple data-dependent initialization procedure, that sets the weights of a network such that all units in the network train at roughly the same rate, avoiding vanishing or exploding gradients. Our initialization matches the current state-of-the-art unsupervised or self-supervised pre-training methods on standard computer vision tasks, such as image classification and object detection, while being roughly three orders of magnitude faster. When combined with pre-training methods, our initialization significantly outperforms prior work, narrowing the gap between supervised and unsupervised pre-training.},
	archivePrefix = {arXiv},
	arxivId = {1511.06856},
	author = {Kr{\"{a}}henb{\"{u}}hl, Philipp and Doersch, Carl and Donahue, Jeff and Darrell, Trevor},
	eprint = {1511.06856},
	file = {:Users/ankan/Desktop/Papers/Unsupervised/Data Dependent Initializations of Convolutional Neural Networks.pdf:pdf},
	title = {{Data-dependent Initializations of Convolutional Neural Networks}},
	url = {http://arxiv.org/abs/1511.06856},
	year = {2015}
}
@article{Hinton2015,
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	archivePrefix = {arXiv},
	arxivId = {1503.02531},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	doi = {10.1063/1.4931082},
	eprint = {1503.02531},
	file = {:Users/ankan/Desktop/Papers/Semi-Supervised/Distilling the Knowledge in a Neural Network.pdf:pdf},
	isbn = {3531207857},
	issn = {0022-2488},
	pages = {1--9},
	pmid = {18249735},
	title = {{Distilling the Knowledge in a Neural Network}},
	url = {http://arxiv.org/abs/1503.02531},
	year = {2015}
}
@article{Miyato2015,
	abstract = {We propose local distributional smoothness (LDS), a new notion of smoothness for statistical model that can be used as a regularization term to promote the smoothness of the model distribution. We named the LDS based regularization as virtual adversarial training (VAT). The LDS of a model at an input datapoint is defined as the KL-divergence based robustness of the model distribution against local perturbation around the datapoint. VAT resembles adversarial training, but distinguishes itself in that it determines the adversarial direction from the model distribution alone without using the label information, making it applicable to semi-supervised learning. The computational cost for VAT is relatively low. For neural network, the approximated gradient of the LDS can be computed with no more than three pairs of forward and back propagations. When we applied our technique to supervised and semi-supervised learning for the MNIST dataset, it outperformed all the training methods other than the current state of the art method, which is based on a highly advanced generative model. We also applied our method to SVHN and NORB, and confirmed our method's superior performance over the current state of the art semi-supervised method applied to these datasets.},
	archivePrefix = {arXiv},
	arxivId = {1507.00677},
	author = {Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Nakae, Ken and Ishii, Shin},
	eprint = {1507.00677},
	file = {:Users/ankan/Desktop/Papers/Semi-Supervised/Distributional Smoothing with Virtual Adversarial Training.pdf:pdf},
	title = {{Distributional Smoothing with Virtual Adversarial Training}},
	url = {http://arxiv.org/abs/1507.00677},
	year = {2015}
}
@article{Dai2017,
	abstract = {Semi-supervised learning methods based on generative adversarial networks (GANs) obtained strong empirical results, but it is not clear 1) how the discriminator benefits from joint training with a generator, and 2) why good semi-supervised classification performance and a good generator cannot be obtained at the same time. Theoretically, we show that given the discriminator objective, good semisupervised learning indeed requires a bad generator, and propose the definition of a preferred generator. Empirically, we derive a novel formulation based on our analysis that substantially improves over feature matching GANs, obtaining state-of-the-art results on multiple benchmark datasets.},
	archivePrefix = {arXiv},
	arxivId = {1705.09783},
	author = {Dai, Zihang and Yang, Zhilin and Yang, Fan and Cohen, William W. and Salakhutdinov, Ruslan},
	eprint = {1705.09783},
	file = {:Users/ankan/Desktop/Papers/Semi-Supervised/Good Semi-supervised Learning That Requires a Bad GAN.pdf:pdf},
	issn = {10495258},
	journal = {NIPS},
	title = {{Good Semi-supervised Learning that Requires a Bad GAN}},
	url = {http://arxiv.org/abs/1705.09783},
	year = {2017}
}
@article{Athiwaratkun2018,
	abstract = {Recent advances in deep unsupervised learning have renewed interest in semi-supervised methods, which can learn from both labeled and unlabeled data. Presently the most successful approaches to semi-supervised learning are based on consistency regularization, whereby a model is trained to be robust to small perturbations of its inputs and parameters. We show that consistency regularization leads to flatter but narrower optima. We also show that the test error surface for these methods is approximately convex in regions of weight space traversed by SGD. Inspired by these observations, we propose to train consistency based semi-supervised models with stochastic weight averaging (SWA), a recent method which averages weights along the trajectory of SGD. We also develop fast-SWA, which further accelerates convergence by averaging multiple points within each cycle of a cyclical learning rate schedule. With fast-SWA we achieve the best known semi-supervised results on CIFAR-10 and CIFAR-100 over many different numbers of observed training labels. For example, we achieve 5.0{\%} error on CIFAR-10 with only 4000 labels, compared to 6.28{\%} of the previous best result in the literature. We also improve the best known result from 80{\%} accuracy to 83{\%} for domain adaptation from CIFAR-10 to STL. Finally, we show that with fast-SWA the simple {\$}\backslashPi{\$} model becomes state-of-the-art for large labeled settings.},
	archivePrefix = {arXiv},
	arxivId = {1806.05594},
	author = {Athiwaratkun, Ben and Finzi, Marc and Izmailov, Pavel and Wilson, Andrew Gordon},
	doi = {arXiv:1806.05594v2},
	eprint = {1806.05594},
	file = {:Users/ankan/Desktop/Papers/Semi-Supervised/Improving Consistency-Based Semi-Supervised Learning with Weight Averaging.pdf:pdf},
	pages = {1--20},
	title = {{Improving Consistency-Based Semi-Supervised Learning with Weight Averaging}},
	url = {http://arxiv.org/abs/1806.05594},
	year = {2018}
}
@article{Wang2015,
	abstract = {Localizing objects in cluttered backgrounds is challenging under large-scale weakly supervised conditions. Due to the cluttered image condition, objects usually have large ambiguity with backgrounds. Besides, there is also a lack of effective algorithm for large-scale weakly supervised localization in cluttered backgrounds. However, backgrounds contain useful latent information, e.g., the sky in the aeroplane class. If this latent information can be learned, object-background ambiguity can be largely reduced and background can be suppressed effectively. In this paper, we propose the latent category learning (LCL) in large-scale cluttered conditions. LCL is an unsupervised learning method which requires only image-level class labels. First, we use the latent semantic analysis with semantic object representation to learn the latent categories, which represent objects, object parts or backgrounds. Second, to determine which category contains the target object, we propose a category selection strategy by evaluating each category's discrimination. Finally, we propose the online LCL for use in large-scale conditions. Evaluation on the challenging PASCAL Visual Object Class (VOC) 2007 and the large-scale imagenet large-scale visual recognition challenge 2013 detection data sets shows that the method can improve the annotation precision by 10{\%} over previous methods. More importantly, we achieve the detection precision which outperforms previous results by a large margin and can be competitive to the supervised deformable part model 5.0 baseline on both data sets.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1503.00949v1},
	author = {Wang, Chong and Huang, Kaiqi and Ren, Weiqiang and Zhang, Junge and Maybank, Steve},
	doi = {10.1109/TIP.2015.2396361},
	eprint = {arXiv:1503.00949v1},
	file = {:Users/ankan/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2015 - Large-Scale Weakly Supervised Object Localization via Latent Category Learning.pdf:pdf},
	isbn = {978-3-319-10598-7},
	issn = {10577149},
	journal = {IEEE Transactions on Image Processing},
	keywords = {large-scale,latent semantic analysis,object localization,weakly supervised learning},
	number = {4},
	pages = {1371--1385},
	title = {{Large-Scale Weakly Supervised Object Localization via Latent Category Learning}},
	volume = {24},
	year = {2015}
}
@article{Kim,
	abstract = {We address the problem of large scale image geo-localization where the location of an image is estimated by identifying geo-tagged reference images depicting the same place. We propose a novel model for learning image rep-resentations that integrates context-aware feature reweight-ing in order to effectively focus on regions that positively contribute to geo-localization. In particular, we introduce a Contextual Reweighting Network (CRN) that predicts the importance of each region in the feature map based on the image context. Our model is learned end-to-end for the im-age geo-localization task, and requires no annotation other than image geo-tags for training. In experimental results, the proposed approach significantly outperforms the previ-ous state-of-the-art on the standard geo-localization bench-mark datasets.We also demonstrate that our CRN discovers task-relevant contexts without any additional supervision.},
	author = {Kim, Hyo Jin and Dunn, Enrique and Frahm, Jan-Michael},
	title = {{Learned Contextual Feature Reweighting for Image Geo-Localization}},
	url = {https://hyojin.web.unc.edu/files/2017/06/CVPR2017{\_}0780.pdf}
}
@article{Wei,
	author = {Wei, Donglai and Lim, Jospeh and Zisserman, Andrew and Freeman, William T},
	doi = {10.1109/CVPR.2018.00840},
	file = {:Users/ankan/Desktop/Papers/Unsupervised/Learning and Using the Arrow of Time.pdf:pdf},
	number = {c},
	title = {{Learning and Using the Arrow of Time}}
}
@article{CAM,
	abstract = {In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1{\%} top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2{\%} top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them},
	archivePrefix = {arXiv},
	arxivId = {1512.04150},
	author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
	doi = {10.1109/CVPR.2016.319},
	eprint = {1512.04150},
	file = {:Users/ankan/Desktop/Papers/Class Activation Maps.pdf:pdf},
	isbn = {978-1-4673-8851-1},
	issn = {0920-5691},
	pages = {2921--2929},
	title = {{Learning Deep Features for Discriminative Localization}},
	url = {http://arxiv.org/abs/1512.04150},
	year = {2015}
}
@article{Pathak2017,
	abstract = {This paper presents a novel yet intuitive approach to unsupervised feature learning. Inspired by the human visual system, we explore whether low-level motion-based grouping cues can be used to learn an effective visual representation. Specifically, we use unsupervised motion-based segmentation on videos to obtain segments, which we use as 'pseudo ground truth' to train a convolutional network to segment objects from a single frame. Given the extensive evidence that motion plays a key role in the development of the human visual system, we hope that this straightforward approach to unsupervised learning will be more effective than cleverly designed 'pretext' tasks studied in the literature. Indeed, our extensive experiments show that this is the case. When used for transfer learning on object detection, our representation significantly outperforms previous unsupervised approaches across multiple settings, especially when training data for the target task is scarce.},
	archivePrefix = {arXiv},
	arxivId = {1612.06370},
	author = {Pathak, Deepak and Girshick, Ross and Doll{\'{a}}r, Piotr and Darrell, Trevor and Hariharan, Bharath},
	doi = {10.1109/CVPR.2017.638},
	eprint = {1612.06370},
	file = {:Users/ankan/Desktop/Papers/Unsupervised/Learning Features by Watching Objects Move.pdf:pdf},
	isbn = {9781538604571},
	issn = {1612.06370},
	journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
	pages = {6024--6033},
	title = {{Learning features by watching objects move}},
	volume = {2017-Janua},
	year = {2017}
}
@article{Jayaraman2015,
	abstract = {Understanding how images of objects and scenes behave in response to specific ego-motions is a crucial aspect of proper visual development, yet existing visual learning methods are conspicuously disconnected from the physical source of their images. We propose to exploit proprioceptive motor signals to provide unsupervised regularization in convolutional neural networks to learn visual representations from egocentric video. Specifically, we enforce that our learned features exhibit equivariance i.e. they respond predictably to transformations associated with distinct ego-motions. With three datasets, we show that our unsupervised feature learning approach significantly outperforms previous approaches on visual recognition and next-best-view prediction tasks. In the most challenging test, we show that features learned from video captured on an autonomous driving platform improve large-scale scene recognition in static images from a disjoint domain.},
	archivePrefix = {arXiv},
	arxivId = {1505.02206},
	author = {Jayaraman, Dinesh and Grauman, Kristen},
	doi = {10.1109/ICCV.2015.166},
	eprint = {1505.02206},
	file = {:Users/ankan/Desktop/Papers/Unsupervised/Learning image representations tied to ego-motion.pdf:pdf},
	isbn = {9781467383912},
	issn = {15505499},
	journal = {Proceedings of the IEEE International Conference on Computer Vision},
	pages = {1413--1421},
	title = {{Learning image representations tied to ego-motion}},
	volume = {2015 Inter},
	year = {2015}
}
@article{Ahn2018,
	abstract = {The deficiency of segmentation labels is one of the main obstacles to semantic segmentation in the wild. To alleviate this issue, we present a novel framework that generates segmentation labels of images given their image-level class labels. In this weakly supervised setting, trained models have been known to segment local discriminative parts rather than the entire object area. Our solution is to propagate such local responses to nearby areas which belong to the same semantic entity. To this end, we propose a Deep Neural Network (DNN) called AffinityNet that predicts semantic affinity between a pair of adjacent image coordinates. The semantic propagation is then realized by random walk with the affinities predicted by AffinityNet. More importantly, the supervision employed to train AffinityNet is given by the initial discriminative part segmentation, which is incomplete as a segmentation annotation but sufficient for learning semantic affinities within small image areas. Thus the entire framework relies only on image-level class labels and does not require any extra data or annotations. On the PASCAL VOC 2012 dataset, a DNN learned with segmentation labels generated by our method outperforms previous models trained with the same level of supervision, and is even as competitive as those relying on stronger supervision.},
	archivePrefix = {arXiv},
	arxivId = {1803.10464},
	author = {Ahn, Jiwoon and Kwak, Suha},
	eprint = {1803.10464},
	file = {:Users/ankan/Desktop/Papers/Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation.pdf:pdf},
	month = {mar},
	title = {{Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation}},
	url = {http://arxiv.org/abs/1803.10464},
	year = {2018}
}
@article{Ramanathan2015,
	abstract = {In this paper, we propose to learn temporal embeddings of video frames for complex video analysis. Large quantities of unlabeled video data can be easily obtained from the Internet. These videos possess the implicit weak label that they are sequences of temporally and semantically coherent images. We leverage this information to learn temporal embeddings for video frames by associating frames with the temporal context that they appear in. To do this, we propose a scheme for incorporating temporal context based on past and future frames in videos, and compare this to other contextual representations. In addition, we show how data augmentation using multi-resolution samples and hard negatives helps to significantly improve the quality of the learned embeddings. We evaluate various design decisions for learning temporal embeddings, and show that our embeddings can improve performance for multiple video tasks such as retrieval, classification, and temporal order recovery in unconstrained Internet video.},
	archivePrefix = {arXiv},
	arxivId = {1505.00315},
	author = {Ramanathan, Vignesh and Tang, Kevin and Mori, Greg and Fei-Fei, Li},
	doi = {10.1109/ICCV.2015.508},
	eprint = {1505.00315},
	file = {:Users/ankan/Desktop/Papers/Unsupervised/Learning Temporal Embeddings for Complex Video Analysis.pdf:pdf},
	isbn = {9781467383912},
	issn = {15505499},
	journal = {Proceedings of the IEEE International Conference on Computer Vision},
	pages = {4471--4479},
	title = {{Learning temporal embeddings for complex video analysis}},
	volume = {2015 Inter},
	year = {2015}
}
@article{Agrawal2015,
	abstract = {The dominant paradigm for feature learning in computer vision relies on training neural networks for the task of object recognition using millions of hand labelled images. Is it possible to learn useful features for a diverse set of visual tasks using any other form of supervision? In biology, living organisms developed the ability of visual perception for the purpose of moving and acting in the world. Drawing inspiration from this observation, in this work we investigate if the awareness of egomotion can be used as a supervisory signal for feature learning. As opposed to the knowledge of class labels, information about egomotion is freely available to mobile agents. We show that given the same number of training images, features learnt using egomotion as supervision compare favourably to features learnt using class-label as supervision on visual tasks of scene recognition, object recognition, visual odometry and keypoint matching.},
	archivePrefix = {arXiv},
	arxivId = {1505.01596},
	author = {Agrawal, Pulkit and Carreira, Joao and Malik, Jitendra},
	doi = {10.1109/ICCV.2015.13},
	eprint = {1505.01596},
	file = {:Users/ankan/Desktop/Papers/Unsupervised/Learning to See by Moving.pdf:pdf},
	isbn = {9781467383912},
	issn = {15505499},
	journal = {Proceedings of the IEEE International Conference on Computer Vision},
	pages = {37--45},
	title = {{Learning to see by moving}},
	volume = {2015 Inter},
	year = {2015}
}
@article{Metz2018,
	abstract = {A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this goal is approached by minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise incidentally. In this work, we propose instead to directly target a later desired task by meta-learning an unsupervised learning rule, which leads to representations useful for that task. Here, our desired task (meta-objective) is the performance of the representation on semi-supervised classification, and we meta-learn an algorithm -- an unsupervised weight update rule -- that produces representations that perform well under this meta-objective. Additionally, we constrain our unsupervised update rule to a be a biologically-motivated, neuron-local function, which enables it to generalize to novel neural network architectures. We show that the meta-learned update rule produces useful features and sometimes outperforms existing unsupervised learning techniques. We further show that the meta-learned unsupervised update rule generalizes to train networks with different widths, depths, and nonlinearities. It also generalizes to train on data with randomly permuted input dimensions and even generalizes from image datasets to a text task.},
	archivePrefix = {arXiv},
	arxivId = {1804.00222},
	author = {Metz, Luke and Maheswaranathan, Niru and Cheung, Brian and Sohl-Dickstein, Jascha},
	eprint = {1804.00222},
	file = {:Users/ankan/Desktop/Papers/Unsupervised/ Learning Unsupervised Learning Rules.pdf:pdf},
	title = {{Learning Unsupervised Learning Rules}},
	url = {http://arxiv.org/abs/1804.00222},
	year = {2018}
}
@article{Winn,
	abstract = {We address the problem of learning object class models and object segmentations from unannotated images. We intro-duce LOCUS (Learning Object Classes with Unsupervised Segmentation) which uses a generative probabilistic model to combine bottom-up cues of color and edge with top-down cues of shape and pose. A key aspect of this model is that the object appearance is allowed to vary from image to im-age, allowing for significant within-class variation. By it-eratively updating the belief in the object's position, size, segmentation and pose, LOCUS avoids making hard deci-sions about any of these quantities and so allows for each to be refined at any stage. We show that LOCUS success-fully learns an object class model from unlabeled images, whilst also giving segmentation accuracies that rival exist-ing supervised methods. Finally, we demonstrate simulta-neous recognition and segmentation in novel images using the learned models for a number of object classes, as well as unsupervised object discovery and tracking in video.},
	author = {Winn, J and Jojic, N},
	title = {{LOCUS: Learning Object Classes with Unsupervised Segmentation}},
	url = {http://www.cs.cmu.edu/{~}./efros/courses/AP06/Papers/winn-iccv-05.pdf}
}
@article{Tarvainen2017,
	abstract = {The recently proposed Temporal Ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets. To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional benefit, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling. Without changing the network architecture, Mean Teacher achieves an error rate of 4.35{\%} on SVHN with 250 labels, outperforming Temporal Ensembling trained with 1000 labels. We also show that a good network architecture is crucial to performance. Combining Mean Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with 4000 labels from 10.55{\%} to 6.28{\%}, and on ImageNet 2012 with 10{\%} of the labels from 35.24{\%} to 9.11{\%}.},
	archivePrefix = {arXiv},
	arxivId = {1703.01780},
	author = {Tarvainen, Antti and Valpola, Harri},
	eprint = {1703.01780},
	file = {:Users/ankan/Desktop/Papers/Semi-Supervised/Mean teachers are better role models- Weight-averaged consistency targets improve semi-supervised deep learning results.pdf:pdf},
	issn = {10495258},
	journal = {NIPS},
	title = {{Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results}},
	url = {http://arxiv.org/abs/1703.01780},
	year = {2017}
}
@article{Wan,
	abstract = {Weakly supervised object detection is a challenging task when provided with image category supervision but re-quired to learn, at the same time, object locations and ob-ject detectors. The inconsistency between the weak supervi-sion and learning objectives introduces randomness to ob-ject locations and ambiguity to detectors. In this paper, a min-entropy latent model (MELM) is proposed for weakly supervised object detection. Min-entropy is used as a met-ric to measure the randomness of object localization dur-ing learning, as well as serving as a model to learn ob-ject locations. It aims to principally reduce the variance of positive instances and alleviate the ambiguity of detectors. MELM is deployed as two sub-models, which respectively discovers and localizes objects by minimizing the global and local entropy. MELM is unified with feature learning and optimized with a recurrent learning algorithm, which progressively transfers the weak supervision to object lo-cations. Experiments demonstrate that MELM significantly improves the performance of weakly supervised detection, weakly supervised localization, and image classification, against the state-of-the-art approaches.},
	author = {Wan, Fang and Wei, Pengxu and Jiao, Jianbin and Han, Zhenjun and Ye, Qixiang},
	file = {:Users/ankan/Desktop/Papers/Min-Entropy Latent Model for Weakly Supervised Object Detection.pdf:pdf},
	title = {{Min-Entropy Latent Model for Weakly Supervised Object Detection}},
	url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2018/papers/Wan{\_}Min-Entropy{\_}Latent{\_}Model{\_}CVPR{\_}2018{\_}paper.pdf}
}
@article{Ge2018,
	abstract = {Supervised object detection and semantic segmentation require object or even pixel level annotations. When there exist image level labels only, it is challenging for weakly supervised algorithms to achieve accurate predictions. The accuracy achieved by top weakly supervised algorithms is still significantly lower than their fully supervised counterparts. In this paper, we propose a novel weakly supervised curriculum learning pipeline for multi-label object recognition, detection and semantic segmentation. In this pipeline, we first obtain intermediate object localization and pixel labeling results for the training images, and then use such results to train task-specific deep networks in a fully supervised manner. The entire process consists of four stages, including object localization in the training images, filtering and fusing object instances, pixel labeling for the training images, and task-specific network training. To obtain clean object instances in the training images, we propose a novel algorithm for filtering, fusing and classifying object instances collected from multiple solution mechanisms. In this algorithm, we incorporate both metric learning and density-based clustering to filter detected object instances. Experiments show that our weakly supervised pipeline achieves state-of-the-art results in multi-label image classification as well as weakly supervised object detection and very competitive results in weakly supervised semantic segmentation on MS-COCO, PASCAL VOC 2007 and PASCAL VOC 2012.},
	archivePrefix = {arXiv},
	arxivId = {1802.09129},
	author = {Ge, Weifeng and Yang, Sibei and Yu, Yizhou},
	eprint = {1802.09129},
	file = {:Users/ankan/Desktop/Papers/Multi-Evidence Filtering and Fusion for Multi-Label Classification, Object Detection and Semantic Segmentation Based on Weakly Supervised Learning.pdf:pdf},
	title = {{Multi-Evidence Filtering and Fusion for Multi-Label Classification, Object Detection and Semantic Segmentation Based on Weakly Supervised Learning}},
	url = {http://arxiv.org/abs/1802.09129},
	year = {2018}
}
@article{Doersch2017,
	abstract = {We investigate methods for combining multiple self-supervised tasks--i.e., supervised tasks where data can be collected without manual labeling--in order to train a single visual representation. First, we provide an apples-to-apples comparison of four different self-supervised tasks using the very deep ResNet-101 architecture. We then combine tasks to jointly train a network. We also explore lasso regularization to encourage the network to factorize the information in its representation, and methods for "harmonizing" network inputs in order to learn a more unified representation. We evaluate all methods on ImageNet classification, PASCAL VOC detection, and NYU depth prediction. Our results show that deeper networks work better, and that combining tasks--even via a naive multi-head architecture--always improves performance. Our best joint network nearly matches the PASCAL performance of a model pre-trained on ImageNet classification, and matches the ImageNet network on NYU depth prediction.},
	archivePrefix = {arXiv},
	arxivId = {1708.07860},
	author = {Doersch, Carl and Zisserman, Andrew},
	doi = {10.1109/ICCV.2017.226},
	eprint = {1708.07860},
	file = {:Users/ankan/Desktop/Papers/Unsupervised/Multi-task Self-Supervised Visual Learning.pdf:pdf},
	isbn = {9781538610329},
	issn = {15505499},
	journal = {Proceedings of the IEEE International Conference on Computer Vision},
	pages = {2070--2079},
	title = {{Multi-task Self-Supervised Visual Learning}},
	volume = {2017-Octob},
	year = {2017}
}
@article{Sajjadi2016,
	abstract = {In this paper we consider the problem of semi-supervised learning with deep Convolutional Neural Networks (ConvNets). Semi-supervised learning is motivated on the observation that unlabeled data is cheap and can be used to improve the accuracy of classifiers. In this paper we propose an unsupervised regularization term that explicitly forces the classifier's prediction for multiple classes to be mutually-exclusive and effectively guides the decision boundary to lie on the low density space between the manifolds corresponding to different classes of data. Our proposed approach is general and can be used with any backpropagation-based learning method. We show through different experiments that our method can improve the object recognition performance of ConvNets using unlabeled data.},
	archivePrefix = {arXiv},
	arxivId = {1606.03141},
	author = {Sajjadi, Mehdi and Javanmardi, Mehran and Tasdizen, Tolga},
	doi = {10.1109/ICIP.2016.7532690},
	eprint = {1606.03141},
	file = {:Users/ankan/Desktop/Papers/Semi-Supervised/Mutual Exclusivity Loss for Semi-supervised Learning.pdf:pdf},
	isbn = {9781467399616},
	issn = {15224880},
	journal = {Proceedings - International Conference on Image Processing, ICIP},
	keywords = {Deep Learning,Neural Networks,Semi-supervised Learning},
	pages = {1908--1912},
	title = {{Mutual exclusivity loss for semi-supervised deep learning}},
	volume = {2016-August},
	year = {2016}
}
@inproceedings{Sajjadi2016a,
	title={Regularization with stochastic transformations and perturbations for deep
		semi-supervised learning},
	author={Sajjadi, Mehdi and Javanmardi, Mehran and Tasdizen, Tolga},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1163--1171},
	year={2016}
}
@article{Chen2013,
	abstract = {We propose NEIL (Never Ending Image Learner), a computer program that runs 24 hours per day and 7 days per week to automatically extract visual knowledge from Internet data. NEIL uses a semi-supervised learning algorithm that jointly discovers common...},
	author = {Chen, Xinlei and Shrivastava, Abhinav and Gupta, Abhinav},
	doi = {10.1109/ICCV.2013.178},
	file = {:Users/ankan/Desktop/Papers/Semi-Supervised/NEIL- Extracting Visual Knowledge from Web Data.pdf:pdf},
	isbn = {9781479928392},
	issn = {1550-5499},
	journal = {Proceedings of the IEEE International Conference on Computer Vision},
	keywords = {attributes,common sense relationships,macro vision,never ending learning,object detection,scene classification,semi-supervised learning,visual knowledge base},
	pages = {1409--1416},
	title = {{NEIL: Extracting visual knowledge from web data}},
	year = {2013}
}
@article{Bau2017,
	abstract = {We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.},
	archivePrefix = {arXiv},
	arxivId = {1704.05796},
	author = {Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
	doi = {10.1109/CVPR.2017.354},
	eprint = {1704.05796},
	file = {:Users/ankan/Desktop/Papers/Unsupervised/Network Dissection- Quantifying Interpretability of Deep Visual Representations.pdf:pdf},
	isbn = {9781538604571},
	journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
	pages = {3319--3327},
	title = {{Network dissection: Quantifying interpretability of deep visual representations}},
	volume = {2017-Janua},
	year = {2017}
}
@article{Li2010,
	abstract = {A well-built dataset is a necessary starting point for advanced computer vision research. It plays a crucial role in evaluation and provides a continuous challenge to state-of-the-art algorithms. Dataset collection is, however, a tedious and time-consuming task. This paper presents a novel automatic dataset collecting and model learning approach that uses object recognition techniques in an incremental method. The goal of this work is to use the tremendous resources of the web to learn robust object category models in order to detect and search for objects in real-world cluttered scenes. It mimics the human learning process of iteratively accumulating model knowledge and image examples. We adapt a non-parametric graphical model and propose an incremental learning framework. Our algorithm is capable of automatically collecting much larger object category datasets for 22 randomly selected classes from the Caltech 101 dataset. Furthermore, we offer not only more images in each object category dataset, but also a robust object model and meaningful image annotation. Our experiments show that OPTIMOL is capable of collecting image datasets that are superior to Caltech 101 and LabelMe.},
	author = {Li, Li Jia and Fei-Fei, Li},
	doi = {10.1007/s11263-009-0265-6},
	file = {:Users/ankan/Desktop/Papers/Semi-Supervised/OPTIMOL- Automatic Online Picture Collection via Incremental Model Learning.pdf:pdf},
	isbn = {1424411807},
	issn = {09205691},
	journal = {International Journal of Computer Vision},
	keywords = {Annotation,Classification,Dataset collection,Generative model,Graphical model,Image retrieval,Incremental learning,Internet vision,Non-parametric model,Object localization,Object recognition,Semi-supervised learning,Topic model},
	number = {2},
	pages = {147--168},
	title = {{OPTIMOL: Automatic online picture collection via incremental model learning}},
	volume = {88},
	year = {2010}
}
@article{Singh2017,
	abstract = {We present R-FCN-3000, a large-scale real-time object detector in which objectness detection and classification are decoupled. To obtain the detection score for an RoI, we multiply the objectness score with the fine-grained classification score. Our approach is a modification of the R-FCN architecture in which position-sensitive filters are shared across different object classes for performing localization. For fine-grained classification, these position-sensitive filters are not needed. R-FCN-3000 obtains an mAP of 34.9{\%} on the ImageNet detection dataset and outperforms YOLO-9000 by 18{\%} while processing 30 images per second. We also show that the objectness learned by R-FCN-3000 generalizes to novel classes and the performance increases with the number of training object classes - supporting the hypothesis that it is possible to learn a universal objectness detector. Code will be made available.},
	archivePrefix = {arXiv},
	arxivId = {1712.01802},
	author = {Singh, Bharat and Li, Hengduo and Sharma, Abhishek and Davis, Larry S.},
	eprint = {1712.01802},
	title = {{R-FCN-3000 at 30fps: Decoupling Detection and Classification}},
	url = {http://arxiv.org/abs/1712.01802},
	year = {2017}
}
@article{Noroozi2017,
	abstract = {We introduce a novel method for representation learning that uses an artificial supervision signal based on counting visual primitives. This supervision signal is obtained from an equivariance relation, which does not require any manual annotation. We relate transformations of images to transformations of the representations. More specifically, we look for the representation that satisfies such relation rather than the transformations that match a given representation. In this paper, we use two image transformations in the context of counting: scaling and tiling. The first transformation exploits the fact that the number of visual primitives should be invariant to scale. The second transformation allows us to equate the total number of visual primitives in each tile to that in the whole image. These two transformations are combined in one constraint and used to train a neural network with a contrastive loss. The proposed task produces representations that perform on par or exceed the state of the art in transfer learning benchmarks.},
	archivePrefix = {arXiv},
	arxivId = {1708.06734},
	author = {Noroozi, Mehdi and Pirsiavash, Hamed and Favaro, Paolo},
	doi = {10.1109/ICCV.2017.628},
	eprint = {1708.06734},
	file = {:Users/ankan/Desktop/Papers/Unsupervised/ Representation Learning by Learning to Count.pdf:pdf},
	isbn = {9781538610329},
	issn = {15505499},
	title = {{Representation Learning by Learning to Count}},
	url = {http://arxiv.org/abs/1708.06734},
	year = {2017}
}
@article{Sun2017,
	abstract = {The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.},
	archivePrefix = {arXiv},
	arxivId = {1707.02968},
	author = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
	doi = {10.1109/ICCV.2017.97},
	eprint = {1707.02968},
	file = {:Users/ankan/Desktop/Papers/Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.pdf:pdf},
	isbn = {9781538610329},
	issn = {15505499},
	journal = {Proceedings of the IEEE International Conference on Computer Vision},
	pages = {843--852},
	title = {{Revisiting Unreasonable Effectiveness of Data in Deep Learning Era}},
	volume = {2017-Octob},
	year = {2017}
}
@article{Grandvalet2005,
	abstract = {We consider the semi-supervised learning problem, where a decision rule is to be learned from labeled and unlabeled data. In this framework, we motivate minimum entropy regularization, which enables to incorporate unlabeled data in the standard supervised learning. Our approach includes other approaches to the semi-supervised problem as particular or limiting cases. A series of experiments illustrates that the proposed solution benefits from unlabeled data. The method challenges mixture models when the data are sampled from the distribution class spanned by the generative model. The performances are definitely in favor of minimum entropy regularization when generative models are misspecified, and the weighting of unlabeled data provides robustness to the violation of the ¬ìcluster assumption¬î. Finally, we also illustrate that the method can also be far superior to manifold learning in high dimension spaces.},
	author = {Grandvalet, Yves and Bengio, Yoshua},
	file = {:Users/ankan/Desktop/Papers/Semi-Supervised/Semi-supervised Learning by Entropy Minimization.pdf:pdf},
	isbn = {0262195348},
	issn = {10495258},
	journal = {Advances in Neural Information Processing Systems 17 (NIPS'04)},
	pages = {236--529},
	title = {{Semi-supervised Learning by Entropy Minimization}},
	year = {2005}
}
@article{Zhou2004,
	author = {Zhou, Dengyong and Bousquet, Olivier and Lal, T},
	file = {:Users/ankan/Desktop/Papers/Semi-Supervised/Semi-supervised Learning by Maximizing Smoothness.pdf:pdf},
	journal = {J. of Mach. Learn. {\ldots}},
	keywords = {random walks,regularization,semi-supervised learning,spec-,spreading activation networks,tral graph theory},
	title = {{Semi-supervised learning by maximizing smoothness}},
	url = {http://www.kyb.mpg.de/publications/pdfs/pdf2595.pdf},
	year = {2004}
}
@article{Fergus2009,
	abstract = {With the advent of the Internet it is now possible to collect hundreds of millions of images. These images come with varying degrees of label information. ‚ÄúClean labels‚Äù can be manually obtained on a small fraction, ‚Äúnoisy labels‚Äù may be ex- tracted automatically from surrounding text, while for most images there are no labels at all. Semi-supervised learning is a principled framework for combining these different label sources. However, it scales polynomially with the number of images, making it impractical for use on gigantic collections with hundreds of millions of images and thousands of classes. In this paper we show how to uti- lize recent results in machine learning to obtain highly efficient approximations for semi-supervised learning that are linear in the number of images. Specifically, we use the convergence of the eigenvectors of the normalized graph Laplacian to eigenfunctions of weighted Laplace-Beltrami operators. Our algorithm enables us to apply semi-supervised learning to a database of 80 million images gathered from the Internet.},
	author = {Fergus, Rob and Weiss, Yair and Torralba, Antonio},
	file = {:Users/ankan/Desktop/Papers/Semi-Supervised/Semi-supervised Learning in Gigantic Image Collections.pdf:pdf},
	isbn = {9781615679119},
	journal = {NIPS},
	pages = {522--530},
	title = {{Semi-Supervised Learning in Gigantic Image Collections}},
	url = {http://cs.nyu.edu/{~}fergus/pmwiki/pmwiki.php?n=PmWiki.Publications ???},
	year = {2009}
}
@article{Zhu2008,
	archivePrefix = {arXiv},
	arxivId = {1412.6596},
	author = {Zhu, Xiaojin},
	doi = {10.1.1.146.2352},
	eprint = {1412.6596},
	file = {:Users/ankan/Desktop/Papers/Semi-Supervised/Old Semi-Supervised Learning Literature Survey.pdf:pdf},
	isbn = {0769529321},
	issn = {0002-7863},
	journal = {SciencesNew York},
	number = {1530},
	pages = {10},
	pmid = {22175947},
	title = {{Semi-Supervised Learning Literature Survey Contents}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.99.9681{\&}rep=rep1{\&}type=pdf},
	volume = {10},
	year = {2008}
}
@article{Kamnitsas2018,
	abstract = {We present a novel loss function for semi- supervised learning of neural networks with a simple and effective regularization term based on compact clustering of the latent feature space. The key idea is to dynamically create a graph over both labeled and unlabeled training samples using Label Propagation (LP) to capture the underlying structure in the feature space and model its high and low density areas. The regularization attracts similar samples to form compact clusters and repulses dissimilar ones without applying strong forces to unconfident samples. Label confidence is directly obtained via LP in contrast to using predictions from an imperfect classifier as in previous work. We evaluate our approach on three benchmarks and compare to state-of-the art with promising results. Our method can be easily applied to any existing network architecture enabling an effective use of unlabeled data for a wide range of applications.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1806.02679},
	author = {Kamnitsas, Konstantinos and Coelho, Daniel and Loic, De Castro and Folgoc, Le and Walker, Ian and Tanno, Ryutaro and Rueckert, Daniel and Glocker, Ben and Criminisi, Antonio and Nori, Aditya},
	eprint = {arXiv:1806.02679},
	title = {{Semi-supervised learning via compact latent space clustering}},
	year = {2018}
}
@article{Kingma2014,
	abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1406.5298v1},
	author = {Kingma, Dp and Rezende, Dj and Mohamed, S and Welling, Max},
	eprint = {arXiv:1406.5298v1},
	file = {:Users/ankan/Desktop/Papers/Semi-Supervised/Semi-supervised Learning with Deep Generative Model.pdf:pdf},
	issn = {10495258},
	journal = {arXiv preprint arXiv: {\ldots}},
	pages = {1--9},
	title = {{Semi-supervised Learning with Deep Generative Models}},
	url = {http://arxiv.org/abs/1406.5298},
	year = {2014}
}
@article{Rasmus2015,
	abstract = {We combine supervised learning with unsupervised learning in deep neural networks. The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pre-training. Our work builds on the Ladder network proposed by Valpola (2015), which we extend by combining the model with supervision. We show that the resulting model reaches state-of-the-art performance in semi-supervised MNIST and CIFAR-10 classification, in addition to permutation-invariant MNIST classification with all labels.},
	archivePrefix = {arXiv},
	arxivId = {1507.02672},
	author = {Rasmus, Antti and Valpola, Harri and Honkala, Mikko and Berglund, Mathias and Raiko, Tapani},
	doi = {10.1017/CBO9781107415324.004},
	eprint = {1507.02672},
	file = {:Users/ankan/Desktop/Papers/Semi-Supervised/Semi-Supervised Learning with Ladder Networks.pdf:pdf},
	isbn = {9788578110796},
	issn = {10495258},
	pages = {1--9},
	pmid = {25246403},
	title = {{Semi-Supervised Learning with Ladder Networks}},
	url = {http://arxiv.org/abs/1507.02672},
	year = {2015}
}
@article{Rosenberg2007,
	abstract = {The construction of appearance-based object detection systems is time-consuming and difficult because a large number of training examples must be collected and manually labeled in order to capture variations in object appearance. Semi-supervised training is a means for reducing the effort needed to prepare the training set by training the model with a small number of fully labeled examples and an additional set of unlabeled or weakly labeled examples. In this work we present a semi-supervised approach to training object detection systems based on self-training. We implement our approach as a wrapper around the training process of an existing object detector and present empirical results. The key contributions of this empirical study is to demonstrate that a model trained in this manner can achieve results comparable to a model trained in the traditional manner using a much larger set of fully labeled data, and that a training data selection metric that is defined independently of the detector greatly outperforms a selection metric based on the detection confidence generated by the detector.},
	author = {Rosenberg, Chuck and Hebert, Martial and Schneiderman, Henry},
	doi = {10.1109/ACVMOT.2005.107},
	file = {:Users/ankan/Desktop/Papers/Semi-Supervised/Semi-Supervised Self-Training of Object Detection Models.pdf:pdf},
	isbn = {0769522718},
	journal = {Proceedings - Seventh IEEE Workshop on Applications of Computer Vision, WACV 2005},
	pages = {29--36},
	title = {{Semi-supervised self-training of object detection models}},
	year = {2007}
}
@article{Ratle2010,
	author = {Ratle, F},
	file = {:Users/ankan/Desktop/Papers/Semi-Supervised/Semi-Supervised Neural Networks for Efficient Hyperspectral Image Classification.pdf:pdf},
	journal = {Geoscience and Remote  {\ldots}},
	pages = {1--12},
	title = {{Semisupervised neural networks for efficient hyperspectral image classification}},
	url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5411821},
	volume = {XX},
	year = {2010}
}
@article{Misra2016,
	abstract = {In this paper, we present an approach for learning a visual representation from the raw spatiotemporal signals in videos. Our representation is learned without supervision from semantic labels. We formulate our method as an unsupervised sequential verification task, i.e., we determine whether a sequence of frames from a video is in the correct temporal order. With this simple task and no semantic labels, we learn a powerful visual representation using a Convolutional Neural Network (CNN). The representation contains complementary information to that learned from supervised image datasets like ImageNet. Qualitative results show that our method captures information that is temporally varying, such as human pose. When used as pre-training for action recognition, our method gives significant gains over learning without external data on benchmark datasets like UCF101 and HMDB51. To demonstrate its sensitivity to human pose, we show results for pose estimation on the FLIC and MPII datasets that are competitive, or better than approaches using significantly more supervision. Our method can be combined with supervised representations to provide an additional boost in accuracy.},
	archivePrefix = {arXiv},
	arxivId = {1603.08561},
	author = {Misra, Ishan and Zitnick, C. Lawrence and Hebert, Martial},
	doi = {10.1007/978-3-319-46448-0_32},
	eprint = {1603.08561},
	file = {:Users/ankan/Desktop/Papers/Unsupervised/ Shuffle and Learn- Unsupervised Learning using Temporal Order Verification.pdf:pdf},
	isbn = {9783319464473},
	issn = {16113349},
	keywords = {ac-,convolutional neural networks,pose estimation,sequence verification,tion recognition,unsupervised learning,videos},
	pmid = {10463930},
	title = {{Shuffle and Learn: Unsupervised Learning using Temporal Order Verification}},
	url = {http://arxiv.org/abs/1603.08561},
	year = {2016}
}
@inproceedings{spleap,
	author = {Kulkarni, Praveen and Jurie, Fr{\'{e}}d{\'{e}}ric and Zepeda, Joaquin and P{\'{e}}rez, Patrick and Chevallier, Louis},
	booktitle = {ECCV2},
	number = {i},
	title = {{SPLeaP : Soft Pooling of Learned Parts for Image Classification}},
	year = {2016}
}
@article{Zhang2017,
	abstract = {We propose split-brain autoencoders, a straightforward modification of the traditional autoencoder architecture, for unsupervised representation learning. The method adds a split to the network, resulting in two disjoint sub-networks. Each sub-network is trained to perform a difficult task -- predicting one subset of the data channels from another. Together, the sub-networks extract features from the entire input signal. By forcing the network to solve cross-channel prediction tasks, we induce a representation within the network which transfers well to other, unseen tasks. This method achieves state-of-the-art performance on several large-scale transfer learning benchmarks.},
	archivePrefix = {arXiv},
	arxivId = {1611.09842},
	author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A.},
	doi = {10.1109/CVPR.2017.76},
	eprint = {1611.09842},
	file = {:Users/ankan/Desktop/Papers/Unsupervised/Split-Brain Autoencoders- Unsupervised Learning by Cross-Channel Prediction.pdf:pdf},
	isbn = {9781538604571},
	journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
	pages = {645--654},
	title = {{Split-brain autoencoders: Unsupervised learning by cross-channel prediction}},
	volume = {2017-Janua},
	year = {2017}
}
@article{Jenatton2009,
	archivePrefix = {arXiv},
	arxivId = {0909.1440},
	author = {Jenatton, Rodolphe and Obozinski, Guillaume and Bach, Francis},
	eprint = {0909.1440},
	month = {sep},
	title = {{Structured Sparse Principal Component Analysis}},
	url = {http://arxiv.org/abs/0909.1440},
	year = {2009}
}
@article{Laine2016,
	abstract = {In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44{\%} to 7.05{\%} in SVHN with 500 labels and from 18.63{\%} to 16.55{\%} in CIFAR-10 with 4000 labels, and further to 5.12{\%} and 12.16{\%} by enabling the standard augmentations. We additionally obtain a clear improvement in CIFAR-100 classification accuracy by using random images from the Tiny Images dataset as unlabeled extra inputs during training. Finally, we demonstrate good tolerance to incorrect labels.},
	archivePrefix = {arXiv},
	arxivId = {1610.02242},
	author = {Laine, Samuli and Aila, Timo},
	eprint = {1610.02242},
	file = {:Users/ankan/Desktop/Papers/Semi-Supervised/Temporal Ensembling for Semi-Supervised Learning.pdf:pdf},
	number = {2015},
	pages = {1--13},
	title = {{Temporal Ensembling for Semi-Supervised Learning}},
	url = {http://arxiv.org/abs/1610.02242},
	year = {2016}
}
@article{Sermanet2017,
	abstract = {We propose a self-supervised approach for learning representations and robotic behaviors entirely from unlabeled videos recorded from multiple viewpoints, and study how this representation can be used in two robotic imitation settings: imitating object interactions from videos of humans, and imitating human poses. Imitation of human behavior requires a viewpoint-invariant representation that captures the relationships between end-effectors (hands or robot grippers) and the environment, object attributes, and body pose. We train our representations using a metric learning loss, where multiple simultaneous viewpoints of the same observation are attracted in the embedding space, while being repelled from temporal neighbors which are often visually similar but functionally different. In other words, the model simultaneously learns to recognize what is common between different-looking images, and what is different between similar-looking images. This signal causes our model to discover attributes that do not change across viewpoint, but do change across time, while ignoring nuisance variables such as occlusions, motion blur, lighting and background. We demonstrate that this representation can be used by a robot to directly mimic human poses without an explicit correspondence, and that it can be used as a reward function within a reinforcement learning algorithm. While representations are learned from an unlabeled collection of task-related videos, robot behaviors such as pouring are learned by watching a single 3rd-person demonstration by a human. Reward functions obtained by following the human demonstrations under the learned representation enable efficient reinforcement learning that is practical for real-world robotic systems. Video results, open-source code and dataset are available at https://sermanet.github.io/imitate},
	archivePrefix = {arXiv},
	arxivId = {1704.06888},
	author = {Sermanet, Pierre and Lynch, Corey and Hsu, Jasmine and Levine, Sergey},
	doi = {10.1109/CVPRW.2017.69},
	eprint = {1704.06888},
	file = {:Users/ankan/Desktop/Papers/Unsupervised/Time-Contrastive Networks- Self-Supervised Learning from Video.pdf:pdf},
	isbn = {9781538607336},
	issn = {21607516},
	journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
	pages = {486--487},
	title = {{Time-Contrastive Networks: Self-Supervised Learning from Multi-view Observation}},
	volume = {2017-July},
	year = {2017}
}
@article{Vondrick2018,
	abstract = {We use large amounts of unlabeled video to learn models for visual tracking without manual human supervision. We leverage the natural temporal coherency of color to create a model that learns to colorize gray-scale videos by copying colors from a reference frame. Quantitative and qualitative experiments suggest that this task causes the model to automatically learn to track visual regions. Although the model is trained without any ground-truth labels, our method learns to track well enough to outperform optical flow based methods. Finally, our results suggest that failures to track are correlated with failures to colorize, indicating that advancing video colorization may further improve self-supervised visual tracking.},
	archivePrefix = {arXiv},
	arxivId = {1806.09594},
	author = {Vondrick, Carl and Shrivastava, Abhinav and Fathi, Alireza and Guadarrama, Sergio and Murphy, Kevin},
	eprint = {1806.09594},
	file = {:Users/ankan/Desktop/Papers/Unsupervised/Tracking Emerges by Colorizing Videos.pdf:pdf},
	keywords = {colorization,self-supervised learning,tracking,video},
	title = {{Tracking Emerges by Colorizing Videos}},
	url = {http://arxiv.org/abs/1806.09594},
	year = {2018}
}
@article{Wang2017,
	abstract = {Learning visual representations with self-supervised learning has become popular in computer vision. The idea is to design auxiliary tasks where labels are free to obtain. Most of these tasks end up providing data to learn specific kinds of invariance useful for recognition. In this paper, we propose to exploit different self-supervised approaches to learn representations invariant to (i) inter-instance variations (two objects in the same class should have similar features) and (ii) intra-instance variations (viewpoint, pose, deformations, illumination, etc). Instead of combining two approaches with multi-task learning, we argue to organize and reason the data with multiple variations. Specifically, we propose to generate a graph with millions of objects mined from hundreds of thousands of videos. The objects are connected by two types of edges which correspond to two types of invariance: "different instances but a similar viewpoint and category" and "different viewpoints of the same instance". By applying simple transitivity on the graph with these edges, we can obtain pairs of images exhibiting richer visual invariance. We use this data to train a Triplet-Siamese network with VGG16 as the base architecture and apply the learned representations to different recognition tasks. For object detection, we achieve 63.2{\%} mAP on PASCAL VOC 2007 using Fast R-CNN (compare to 67.3{\%} with ImageNet pre-training). For the challenging COCO dataset, our method is surprisingly close (23.5{\%}) to the ImageNet-supervised counterpart (24.4{\%}) using the Faster R-CNN framework. We also show that our network can perform significantly better than the ImageNet network in the surface normal estimation task.},
	archivePrefix = {arXiv},
	arxivId = {1708.02901},
	author = {Wang, Xiaolong and He, Kaiming and Gupta, Abhinav},
	doi = {10.1109/ICCV.2017.149},
	eprint = {1708.02901},
	file = {:Users/ankan/Desktop/Papers/Unsupervised/Transitive Invariance for Self-supervised Visual Representation Learning.pdf:pdf},
	isbn = {9781538610329},
	issn = {15505499},
	journal = {Proceedings of the IEEE International Conference on Computer Vision},
	pages = {1338--1347},
	title = {{Transitive Invariance for Self-Supervised Visual Representation Learning}},
	volume = {2017-Octob},
	year = {2017}
}
@article{Li2017,
	abstract = {Generative Adversarial Nets (GANs) have shown promise in image generation and semi-supervised learning (SSL). However, existing GANs in SSL have two problems: (1) the generator and the discriminator (i.e. the classifier) may not be optimal at the same time; and (2) the generator cannot control the semantics of the generated samples. The problems essentially arise from the two-player formulation, where a single discriminator shares incompatible roles of identifying fake samples and predicting labels and it only estimates the data without considering the labels. To address the problems, we present triple generative adversarial net (Triple-GAN), which consists of three players---a generator, a discriminator and a classifier. The generator and the classifier characterize the conditional distributions between images and labels, and the discriminator solely focuses on identifying fake image-label pairs. We design compatible utilities to ensure that the distributions characterized by the classifier and the generator both converge to the data distribution. Our results on various datasets demonstrate that Triple-GAN as a unified model can simultaneously (1) achieve the state-of-the-art classification results among deep generative models, and (2) disentangle the classes and styles of the input and transfer smoothly in the data space via interpolation in the latent space class-conditionally.},
	archivePrefix = {arXiv},
	arxivId = {1703.02291},
	author = {Li, Chongxuan and Xu, Kun and Zhu, Jun and Zhang, Bo},
	eprint = {1703.02291},
	file = {:Users/ankan/Desktop/Papers/Semi-Supervised/Triple Generative Adversarial Nets.pdf:pdf},
	issn = {10495258},
	journal = {NIPS},
	title = {{Triple Generative Adversarial Nets}},
	url = {http://arxiv.org/abs/1703.02291},
	year = {2017}
}
@article{Springenberg2015,
	abstract = {In this paper we present a method for learning a discriminative classifier from unlabeled or partially labeled data. Our approach is based on an objective function that trades-off mutual information between observed examples and their predicted categorical class distribution, against robustness of the classifier to an adversarial generative model. The resulting algorithm can either be interpreted as a natural generalization of the generative adversarial networks (GAN) framework or as an extension of the regularized information maximization (RIM) framework to robust classification against an optimal adversary. We empirically evaluate our method - which we dub categorical generative adversarial networks (or CatGAN) - on synthetic data as well as on challenging image classification tasks, demonstrating the robustness of the learned classifiers. We further qualitatively assess the fidelity of samples generated by the adversarial generator that is learned alongside the discriminative classifier, and identify links between the CatGAN objective and discriminative clustering algorithms (such as RIM).},
	archivePrefix = {arXiv},
	arxivId = {1511.06390},
	author = {Springenberg, Jost Tobias},
	eprint = {1511.06390},
	file = {:Users/ankan/Desktop/Papers/Semi-Supervised/Unsupervised and Semi-supervised Learning with Categorical GANs.pdf:pdf},
	number = {2009},
	pages = {1--20},
	title = {{Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks}},
	url = {http://arxiv.org/abs/1511.06390},
	year = {2015}
}
@article{Zhou1851,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1704.07813v2},
	author = {Zhou, Tinghui and Berkeley, U C and Brown, Matthew and Snavely, Noah and Lowe, David G},
	eprint = {arXiv:1704.07813v2},
	file = {:Users/ankan/Desktop/Papers/Unsupervised/Unsupervised Learning of Depth and Ego-Motion from Video.pdf:pdf},
	pages = {1851--1860},
	title = {{Unsupervised Learning of Depth and Ego-Motion from Video}},
	year = {1851}
}
@article{Thewlis2017,
	abstract = {Learning automatically the structure of object categories remains an important open problem in computer vision. In this paper, we propose a novel unsupervised approach that can discover and learn landmarks in object categories, thus characterizing their structure. Our approach is based on factorizing image deformations, as induced by a viewpoint change or an object deformation, by learning a deep neural network that detects landmarks consistently with such visual effects. Furthermore, we show that the learned landmarks establish meaningful correspondences between different object instances in a category without having to impose this requirement explicitly. We assess the method qualitatively on a variety of object types, natural and man-made. We also show that our unsupervised landmarks are highly predictive of manually-annotated landmarks in face benchmark datasets, and can be used to regress these with a high degree of accuracy.},
	archivePrefix = {arXiv},
	arxivId = {1705.02193},
	author = {Thewlis, James and Bilen, Hakan and Vedaldi, Andrea},
	doi = {10.1109/ICCV.2017.348},
	eprint = {1705.02193},
	file = {:Users/ankan/Desktop/Papers/Unsupervised/Unsupervised learning of object landmarks by factorized spatial embeddings.pdf:pdf},
	isbn = {9781538610329},
	issn = {15505499},
	journal = {Proceedings of the IEEE International Conference on Computer Vision},
	pages = {3229--3238},
	title = {{Unsupervised Learning of Object Landmarks by Factorized Spatial Embeddings}},
	volume = {2017-Octob},
	year = {2017}
}
@article{Noroozi2016,
	abstract = {In this paper we study the problem of image representation learning without human annotation. By following the principles of self-supervision, we build a convolutional neural network (CNN) that can be trained to solve Jigsaw puzzles as a pretext task, which requires no manual labeling, and then later repurposed to solve object classification and detection. To maintain the compatibility across tasks we introduce the context-free network (CFN), a siamese-ennead CNN. The CFN takes image tiles as input and explicitly limits the receptive field (or context) of its early processing units to one tile at a time. We show that the CFN includes fewer parameters than AlexNet while preserving the same semantic learning capabilities. By training the CFN to solve Jigsaw puzzles, we learn both a feature mapping of object parts as well as their correct spatial arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. Our proposed method for learning visual representations outperforms state of the art methods in several transfer learning benchmarks.},
	archivePrefix = {arXiv},
	arxivId = {1603.09246},
	author = {Noroozi, Mehdi and Favaro, Paolo},
	doi = {10.1007/978-3-319-46466-4_5},
	eprint = {1603.09246},
	file = {:Users/ankan/Desktop/Papers/Unsupervised/Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles.pdf:pdf},
	isbn = {9783319464657},
	issn = {16113349},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	keywords = {Feature transfer,Image representation learning,Self-supervised learning,Unsupervised learning},
	pages = {69--84},
	pmid = {10463930},
	title = {{Unsupervised learning of visual representations by solving jigsaw puzzles}},
	volume = {9910 LNCS},
	year = {2016}
}
@article{Wang2015a,
	abstract = {Is strong supervision necessary for learning a good visual representation? Do we really need millions of semantically-labeled images to train a Convolutional Neural Network (CNN)? In this paper, we present a simple yet surprisingly powerful approach for unsupervised learning of CNN. Specifically, we use hundreds of thousands of unlabeled videos from the web to learn visual representations. Our key idea is that visual tracking provides the supervision. That is, two patches connected by a track should have similar visual representation in deep feature space since they probably belong to the same object or object part. We design a Siamese-triplet network with a ranking loss function to train this CNN representation. Without using a single image from ImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train an ensemble of unsupervised networks that achieves 52{\%} mAP (no bounding box regression). This performance comes tantalizingly close to its ImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4{\%}. We also show that our unsupervised network can perform competitively in other tasks such as surface-normal estimation.},
	archivePrefix = {arXiv},
	arxivId = {1505.00687},
	author = {Wang, Xiaolong and Gupta, Abhinav},
	doi = {10.1109/ICCV.2015.320},
	eprint = {1505.00687},
	file = {:Users/ankan/Desktop/Papers/Unsupervised/Unsupervised Learning of Visual Representations using Videos.pdf:pdf},
	isbn = {9781467383912},
	issn = {15505499},
	title = {{Unsupervised Learning of Visual Representations using Videos}},
	url = {http://arxiv.org/abs/1505.00687},
	year = {2015}
}
@article{Gidaris2018,
	abstract = {Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4{\%} that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet .},
	archivePrefix = {arXiv},
	arxivId = {1803.07728},
	author = {Gidaris, Spyros and Singh, Praveer and Komodakis, Nikos},
	doi = {arXiv:1803.07728v1},
	eprint = {1803.07728},
	file = {:Users/ankan/Desktop/Papers/Unsupervised/Unsupervised Representation Learning by Predicting Image Rotations .pdf:pdf},
	number = {2016},
	pages = {1--16},
	title = {{Unsupervised Representation Learning by Predicting Image Rotations}},
	url = {http://arxiv.org/abs/1803.07728},
	year = {2018}
}
@article{Radford2015,
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	archivePrefix = {arXiv},
	arxivId = {1511.06434},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	doi = {10.1051/0004-6361/201527329},
	eprint = {1511.06434},
	journal = {arXiv},
	pages = {1--15},
	title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
	url = {http://arxiv.org/abs/1511.06434},
	year = {2015}
}
@article{Gupta2015b,
	abstract = {This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image in the collection and train a discriminative model to predict their relative position within the image. We argue that doing well on this task will require the model to learn about the layout of visual objects and object parts. We demonstrate that the feature representation learned using this within-image context prediction task is indeed able to capture visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned features, when used as pre-training for the R-CNN object detection pipeline, provide a significant boost over random initialization on Pascal object detection, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1505.05192v1},
	author = {Gupta, Abhinav and Efros, Alexei a},
	doi = {10.1109/ICCV.2015.167},
	eprint = {arXiv:1505.05192v1},
	file = {:Users/ankan/Desktop/Papers/Unsupervised/Unsupervised Visual Representation Learning by Context Prediction.pdf:pdf},
	isbn = {978-1-4673-8391-2},
	issn = {978-1-4673-8391-2},
	journal = {arXiv preprint},
	pages = {1422--1430},
	pmid = {903},
	title = {{Unsupervised Visual Representation Learning by Context Prediction}},
	year = {2015}
}
@article{Miyato2017,
	abstract = {We propose a new regularization method based on virtual adversarial loss: a new measure of local smoothness of the output distribution. Virtual adversarial loss is defined as the robustness of the model's posterior distribution against local perturbation around each input data point. Our method is similar to adversarial training, but differs from adversarial training in that it determines the adversarial direction based only on the output distribution and that it is applicable to a semi-supervised setting. Because the directions in which we smooth the model are virtually adversarial, we call our method virtual adversarial training (VAT). The computational cost of VAT is relatively low. For neural networks, the approximated gradient of virtual adversarial loss can be computed with no more than two pairs of forward and backpropagations. In our experiments, we applied VAT to supervised and semi-supervised learning on multiple benchmark datasets. With additional improvement based on entropy minimization principle, our VAT achieves the state-of-the-art performance on SVHN and CIFAR-10 for semi-supervised learning tasks.},
	archivePrefix = {arXiv},
	arxivId = {1704.03976},
	author = {Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Ishii, Shin},
	eprint = {1704.03976},
	file = {:Users/ankan/Desktop/Papers/Semi-Supervised/Virtual Adversarial Training- A Regularization Method for Supervised and Semi-Supervised Learning.pdf:pdf},
	pages = {1--16},
	title = {{Virtual Adversarial Training: a Regularization Method for Supervised and Semi-supervised Learning}},
	url = {http://arxiv.org/abs/1704.03976},
	year = {2017}
}
@article{Shi2015,
	abstract = {Weakly supervised object detection, is a challenging task, where the training procedure involves learning at the same time both, the model appearance and the object location in each image. The classical approach to solve this problem is to consider the location of the object of interest in each image as a latent variable and minimize the loss generated by such latent variable during learning. However, as learning appearance and localization are two interconnected tasks, the optimization is not convex and the procedure can easily get stuck in a poor local minimum, i.e. the algorithm ‚Äúmisses‚Äù the object in some images. In this paper, we help the optimization to get close to the global minimum by enforcing a ‚Äúsoft‚Äù similarity between each possible location in the image and a reduced set of ‚Äúexemplars‚Äù, or clusters, learned with a convex formulation in the training images. The help is effective because it comes from a different and smooth source of information that is not directly connected with the main task. Results show that our method improves a strong baseline based on convolutional neural network features by more than 4 points without any additional features or extra computation at testing time but only adding a small increment of the training time due to the convex clustering.},
	archivePrefix = {arXiv},
	arxivId = {1703.08000},
	author = {Shi, Miaojing and Caesar, Holger and Ferrari, Vittorio},
	doi = {10.1109/CVPR.2015.7298711},
	eprint = {1703.08000},
	file = {:Users/ankan/Desktop/Papers/Weakly Supervised Object Detection with Convex Clustering.pdf:pdf},
	isbn = {9781467369640},
	issn = {10636919},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	month = {mar},
	pages = {1081--1089},
	title = {{Weakly supervised object detection with convex clustering}},
	url = {http://arxiv.org/abs/1703.08000},
	volume = {07-12-June},
	year = {2015}
}
@article{Bilen2017,
	abstract = {We propose to help weakly supervised object localization for classes where location annotations are not available, by transferring things and stuff knowledge from a source set with available annotations. The source and target classes might share similar appearance (e.g. bear fur is similar to cat fur) or appear against similar background (e.g. horse and sheep appear against grass). To exploit this, we acquire three types of knowledge from the source set: a segmentation model trained on both thing and stuff classes; similarity relations between target and source classes; and co-occurrence relations between thing and stuff classes in the source. The segmentation model is used to generate thing and stuff segmentation maps on a target image, while the class similarity and co-occurrence knowledge help refining them. We then incorporate these maps as new cues into a multiple instance learning framework (MIL), propagating the transferred knowledge from the pixel level to the object proposal level. In extensive experiments, we conduct our transfer from the PASCAL Context dataset (source) to the ILSVRC, COCO and PASCAL VOC 2007 datasets (targets). We evaluate our transfer across widely different thing classes, including some that are not similar in appearance, but appear against similar background. The results demonstrate significant improvement over standard MIL, and we outperform the state-of-the-art in the transfer setting.},
	archivePrefix = {arXiv},
	arxivId = {1703.08000},
	author = {Bilen, Hakan and Pedersoli, Marco and Tuytelaars, Tinne},
	doi = {10.1109/ICCV.2017.366},
	eprint = {1703.08000},
	file = {:Users/ankan/Desktop/Papers/Weakly Supervised Object Localization Using Things and Stuff Transfer.pdf:pdf},
	isbn = {9781538610329},
	issn = {15505499},
	journal = {Proceedings of the IEEE International Conference on Computer Vision},
	number = {2},
	pages = {3401--3410},
	pmid = {4520227},
	title = {{Weakly Supervised Object Localization Using Things and Stuff Transfer}},
	url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2015/papers/Bilen{\_}Weakly{\_}Supervised{\_}Object{\_}2015{\_}CVPR{\_}paper.pdf},
	volume = {2017-Octob},
	year = {2017}
}
@article{Pathak2018,
	abstract = {The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both what and how to imitate. We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss. In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference. The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task. Our method is " zero-shot " in the sense that the agent never has access to expert actions during training or for the task demonstration at inference. We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot. Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance. Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/.},
	author = {Pathak, Deepak and Mahmoudieh, Parsa and Luo, Guanghao and Agrawal, Pulkit and Chen, Dian and Shentu, Yide and Shelhamer, Evan and Malik, Jitendra and Efros, Alexei A and Darrell, Trevor},
	file = {:Users/ankan/Desktop/Papers/Unsupervised/Zero-Shot Visual Imitation.pdf:pdf},
	pages = {1--16},
	title = {{Zero-Shot Visual Imitation}},
	url = {https://pathak22.github.io/zeroshot-imitation/resources/iclr18.pdf},
	year = {2018}
}
@article{Zhang2018a,
	abstract = {This paper addresses weakly supervised object detection with only image-level supervision at training stage. Previous approaches train detection models with entire images all at once, making the models prone to being trapped in sub-optimums due to the introduced false positive examples. Unlike them, we propose a zigzag learning strategy to simultaneously discover reliable object instances and prevent the model from overfitting initial seeds. Towards this goal, we first develop a criterion named mean Energy Accumulation Scores (mEAS) to automatically measure and rank localization difficulty of an image containing the target object, and accordingly learn the detector progressively by feeding examples with increasing difficulty. In this way, the model can be well prepared by training on easy examples for learning from more difficult ones and thus gain a stronger detection ability more efficiently. Furthermore, we introduce a novel masking regularization strategy over the high level convolutional feature maps to avoid overfitting initial samples. These two modules formulate a zigzag learning process, where progressive learning endeavors to discover reliable object instances, and masking regularization increases the difficulty of finding object instances properly. We achieve 47.6{\%} mAP on PASCAL VOC 2007, surpassing the state-of-the-arts by a large margin.},
	archivePrefix = {arXiv},
	arxivId = {1804.09466},
	author = {Zhang, Xiaopeng and Feng, Jiashi and Xiong, Hongkai and Tian, Qi},
	eprint = {1804.09466},
	file = {:Users/ankan/Desktop/Papers/Zigzag Learning for Weakly Supervised Object Detection.pdf:pdf},
	month = {apr},
	title = {{Zigzag Learning for Weakly Supervised Object Detection}},
	url = {https://arxiv.org/abs/1804.09466 http://arxiv.org/abs/1804.09466},
	year = {2018}
}

@inproceedings{Salimans2016,
	title={Improved techniques for training gans},
	author={Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and
		Radford, Alec and Chen, Xi},
	booktitle={Advances in Neural Information Processing Systems},
	pages={2234--2242},
	year={2016}
}
@article{Luo2017,
	title={Smooth Neighbors on Teacher Graphs for Semi-supervised Learning},
	author={Luo, Yucen and Zhu, Jun and Li, Mengxi and Ren, Yong and Zhang, Bo},
	journal={arXiv preprint arXiv:1711.00258},
	year={2017}
}
@article{groupsparsity,
	title={Structured variable selection with sparsity-inducing norms},
	author={Jenatton, Rodolphe and Audibert, Jean-Yves and Bach, Francis},
	journal={Journal of Machine Learning Research},
	volume={12},
	number={Oct},
	pages={2777--2824},
	year={2011}
}
@inproceedings{sparsepca,
	title={Structured sparse principal component analysis},
	author={Jenatton, Rodolphe and Obozinski, Guillaume and Bach, Francis},
	booktitle={Proceedings of the Thirteenth International Conference on Artificial
		Intelligence and Statistics},
	pages={366--373},
	year={2010}
}
@inproceedings{subic,
	title={SUBIC: A supervised, structured binary code for image search},
	author={Jain, Himalaya and Zepeda, Joaquin and P{\'e}rez, Patrick and Gribonval, R{\'e}mi},
	booktitle={Proc. Int. Conf. Computer Vision},
	volume={1},
	number={2},
	pages={3},
	year={2017}
}
@inproceedings{densenet,
	  title={Densely Connected Convolutional Networks.},
	    author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
		  booktitle={CVPR},
		    volume={1},
			  number={2},
			    pages={3},
				  year={2017}
}
