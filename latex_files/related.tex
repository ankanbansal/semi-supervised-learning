\section{Related Work}

% Just putting some points and references here. 
\begin{itemize}
	\item Omni-supervised Learning - \cite{Radosavovic2017}
	\begin{itemize}
		\item Special regime of semi-supervised learning
		\item Ensemble the predictions from multi-transform inference in  a way that generates ``hard"
			labels
		\item Use more data from the internet as the unsupervised data
	\end{itemize}

	\item Mutual Exclusivity - \cite{Sajjadi2016}
	\begin{itemize}
		\item They us an unsupervised regularization term that explicitly forces the classifier's
			prediction for multiple classes to be mutually exclusive 
		\item This is the same condition as MEL 
	\end{itemize}
	
	\item Entropy Minimization - \cite{Grandvalet2005}
	\begin{itemize}
		\item Exactly MEL
		\item But applied on a very small scale. No experiments with large data
	\end{itemize}
	
	\item CatGAN - \cite{Springenberg2015}
	\begin{itemize}
		\item See notes/paper for this. This has lots of important material.  
		\item Uses MEL and NBEL as regularizers
	\end{itemize}
	
	\item Ladder Net - \cite{Rasmus2015}
	\begin{itemize}
		\item Read
	\end{itemize}
	
	\item Improved Techniques for Training GANs - \cite{Salimans2016}
	\begin{itemize}
		\item The discriminator of a GAN can be used to classify the images into $K+1$ categories
			instead of just two (real vs fake). These $K+1$ categories include the $K$ classes and the
			extra category for generated data
	\end{itemize}
	
	\item Temporal Ensembling - \cite{Laine2016}
	\begin{itemize}
		\item Self-ensembling where a consensus prediction is formed for the unknown labels using the
			outputs of the network-in-training on different epochs
		\item Used an unsupervised loss weighting function which ramps up with time starting from zero
	\end{itemize}
	
	\item Virtual Adversarial Training - \cite{Miyato2017}
	\begin{itemize}
		\item Local Distributional Smoothness (LDS) - The distribution of classification should be
			smooth around data points
		\item The goal is to improve the smoothness of the model in the neighborhood of all the observed
			inputs
	\end{itemize}
	
	\item Regularization with Stochastic Transformations - \cite{Sajjadi2016a}
	\begin{itemize}
		\item Unsupervised loss function that minimizes the mean squared differences between different
			passes of an individual training sample through the network
		\item Similar flavor to omni-supervised
		\item Can be combined with other losses like Mutual Exclusivity or MEL
	\end{itemize}
	
	\item Compact Latent Space Clustering - \cite{Kamnitsas2018}
	\begin{itemize}
		\item Discusses MEL :(
		\item The method builds on the cluster assumption: samples forming a structure are likely of the
			same class
		\item Enforce a further constraint: All samples of the class should belong to the same structure
		\item See paper/notes
	\end{itemize}
	
	\item BadGAN - \cite{Dai2017}
	\begin{itemize}
		\item Given the discriminator objective, good semi-supervised learning indeed requires a bad
			generator
		\item The paper is extremely boring. So did not really read it. 
	\end{itemize}
	
	\item TripleGAN - \cite{Li2017}
	\begin{itemize}
		\item GANs in SSL have two problems: i.) the gen and disc may not be optimal at the same time;
			ii.) the gen cannot control the semantics of the generated samples
		\item This work uses gen, disc, and classifier
		\item Classifier generates pseudo labels given real data, generator generates pseudo data given
			real labels and discriminator distinguishes whether the data-label pair is from the real
			labeled dataset or not
		\item The disc can access the label info of the unlabeled data from the class and then force the
			gen to generate correct image-label pairs
		\item The desired equilibrium is that the joint distribution defined by the classifier and the
			generator both converge to the true data distribution
		\item Uses MEL and NBEL as regularizers.
	\end{itemize}
	
	\item SNTG - \cite{Luo2017}
	\begin{itemize}
		\item Read notes/paper
	\end{itemize}
\end{itemize}
