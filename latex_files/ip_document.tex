\documentclass[10pt,letterpaper]{article}
\usepackage{times}

\begin{document}

\title{Clustering and Locality Penalties for Semi-Supervised Image Classification}

\author{}

\maketitle

Data-hungry deep neural networks require vast amounts of labeled data. But the annotated data
provided by customers is usually not enough to train models. This scenario corresponds to the custom
labels set up. On the other hand, collecting images
from the internet has never been easier. However, obtaining annotations for these images is
expensive and time-consuming. We need to devise algorithms that can use this large amount of
unlabeled data along with the customer data to train well-performing models. With such algorithms,
teams will be able to use only the existing supervised image set and as many
unsupervised images as they want to train models. We proposed a method which will enable teams to
use the available customer images in a more efficient way. 

Our presented approach for semi-supervised image classification is based on clustering and locality
penalties which encode knowledge about the dataset class distribution and
object locality in the learning objective. The first clustering penalty (called Mean Entropy
Loss, MEL) encourages the classifier to be more confident of its predictions, while the second
(called Negative Batch Entropy Loss, NBEL) prompts the average output distribution to be the
same as the distribution of classes in the training dataset. The proposed locality penalty is
based on group sparsity and encodes the idea that objects in images generally occupy small
regions in the image. We evaluate our approach in the semi-supervised setting on ImageNet and CIFAR-10
datasets.

These unsupervised losses codify common-sense knowledge about
the images. The motivation behind the proposed clustering losses is that images occur in clusters
distributed according to a prior distribution. These are immediately applicable to one-vs-all
classification. The locality loss enforces the common-sense knowledge
that objects occupy small regions in the image. This loss is represented through group sparsity
methods which enjoy translation invariance. Through this loss, this approach provides a means to do
weakly-supervised object detection.
The proposed losses do not require any alterations to existing CNN architectures, and are
compatible with end-to-end training through back-propagation. Another benefit of the approach is
that the proposed losses can be used in a completely unsupervised setting also and ensembles of such
models might provide competitive performance. 
The proposed clustering losses provide a way to deal with class imbalance in the dataset. NBEL
imposes the condition that the average output class distribution should be the same as the dataset
class distribution. The dataset class distribution could be non-uniform or uniform. 


\end{document}
